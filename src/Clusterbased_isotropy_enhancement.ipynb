{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clusterbased_isotropy_enhancement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "auwHg2y-_4Hz"
      },
      "source": [
        "# @title Install Transformers\n",
        "from IPython.display import clear_output\n",
        "!pip install transformers==2.11\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVuK2H63AYhp"
      },
      "source": [
        "# @title Import Requirements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import scipy as sc\n",
        "import math as mt\n",
        "from scipy import cluster as clst\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bxzwn8tAVcL"
      },
      "source": [
        "# @title Loading BERT\n",
        "casing = \"bert-base-uncased\" \n",
        "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\n",
        "\n",
        "config = BertConfig(dropout=0.2, attention_dropout=0.2, )\n",
        "config.output_hidden_states = True\n",
        "\n",
        "model = TFBertModel.from_pretrained(casing, config = config)\n",
        "model.trainable = False\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtU8arrBBMVj"
      },
      "source": [
        "# @title Loading Dataset\n",
        "\n",
        "# STS benchmark\n",
        "#df_train = pd.read_csv('train.tsv', delimiter='\\t' , error_bad_lines=False)\n",
        "df_dev = pd.read_csv('dev.tsv', delimiter='\\t' , error_bad_lines=False)\n",
        "#df_test = pd.read_csv('test.tsv', delimiter='\\t' , error_bad_lines=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdRVMSKFCuza"
      },
      "source": [
        "# @title Required functions\n",
        "\n",
        "def sper_corrcoef(targets, predictions):\n",
        "    \"\"\"Spearman correlation coefficient.\"\"\"\n",
        "    return 100 * sc.stats.spearmanr(targets, predictions)[0]\n",
        "\n",
        "\n",
        "def mean_pooling(inp_representations, representation_dev):\n",
        "    \"\"\" calculating sentence representations by averaging over the tokens.\"\"\"\n",
        "\n",
        "    sum_index=0\n",
        "    sent_representations=[]\n",
        "    for i in range(len(representation_dev)):\n",
        "      sent_representations.append(np.mean(inp_representations[sum_index: sum_index + (len(representation_dev[i]))],axis=0))\n",
        "      sum_index = sum_index + len(representation_dev[i])\n",
        "\n",
        "    return sent_representations\n",
        "\n",
        "\n",
        "def similarity(sent_rep):\n",
        "    \"\"\" calculating cosine similarity between two sentences.\"\"\"\n",
        "  \n",
        "    score = []\n",
        "    l = 0\n",
        "    for i in range(int(len(sent_rep)/2)):\n",
        "        score.append(cosine_similarity(np.reshape(sent_rep[l], (1, 768)),\n",
        "                                      np.reshape(sent_rep[l + 1], (1, 768)))[0][0])\n",
        "        l = l + 2\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def isotropy(representations):\n",
        "    \"\"\"Calculating isotropy of embedding space based on Eq.2\n",
        "           arg:\n",
        "              representations (n_samples, n_dimensions)\n",
        "            \"\"\"\n",
        "\n",
        "    eig_values, eig_vectors = np.linalg.eig(np.matmul(np.transpose(representations),\n",
        "                                                      representations))\n",
        "    max_f = -mt.inf\n",
        "    min_f =  mt.inf\n",
        "\n",
        "    for i in range(eig_vectors.shape[1]):\n",
        "        f = np.matmul(representations, np.expand_dims(eig_vectors[:, i], 1))\n",
        "        f = np.sum(np.exp(f))\n",
        "\n",
        "        min_f = min(min_f, f)\n",
        "        max_f = max(max_f, f)\n",
        "\n",
        "    isotropy = min_f / max_f\n",
        "\n",
        "    return isotropy\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONNd5H3bLjhN"
      },
      "source": [
        "# @title Cluster-based Isotropy Enhancement\n",
        "\n",
        "def cluster_based(representations, n_cluster: int, n_pc: int):\n",
        "  \"\"\" Improving Isotropy of input representations using cluster-based method\n",
        "      Args: \n",
        "            inputs:\n",
        "                  representations: \n",
        "                    input representations numpy array(n_samples, n_dimension)\n",
        "                  n_cluster: \n",
        "                    the number of clusters\n",
        "                  n_pc: \n",
        "                    the number of directions to be discarded\n",
        "            output:\n",
        "                  isotropic representations (n_samples, n_dimension)\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "  centroid, label=clst.vq.kmeans2(representations, n_cluster, minit='points',\n",
        "                                  missing='warn', check_finite=True)\n",
        "  cluster_mean=[]\n",
        "  for i in range(max(label)+1):\n",
        "    sum=np.zeros([1,768]);\n",
        "    for j in np.nonzero(label == i)[0]:\n",
        "      sum=np.add(sum, representations[j])\n",
        "    cluster_mean.append(sum/len(label[label == i]))\n",
        "\n",
        "  zero_mean_representation=[]\n",
        "  for i in range(len(representations)):\n",
        "    zero_mean_representation.append((representations[i])-cluster_mean[label[i]])\n",
        "\n",
        "  cluster_representations={}\n",
        "  for i in range(n_cluster):\n",
        "    cluster_representations.update({i:{}})\n",
        "    for j in range(len(representations)):\n",
        "      if (label[j]==i):\n",
        "        cluster_representations[i].update({j:zero_mean_representation[j]})\n",
        "\n",
        "  cluster_representations2=[]\n",
        "  for j in range(n_cluster):\n",
        "    cluster_representations2.append([])\n",
        "    for key, value in cluster_representations[j].items():\n",
        "      cluster_representations2[j].append(value)\n",
        "\n",
        "  cluster_representations2=np.array(cluster_representations2)\n",
        "\n",
        "\n",
        "  model=PCA()\n",
        "  post_rep=np.zeros((representations.shape[0],representations.shape[1]))\n",
        "\n",
        "  for i in range(n_cluster):\n",
        "      model.fit(np.array(cluster_representations2[i]).reshape((-1,768)))\n",
        "      component = np.reshape(model.components_, (-1, 768))\n",
        "\n",
        "      for index in cluster_representations[i]:\n",
        "        sum_vec = np.zeros((1, 768))\n",
        "\n",
        "        for j in range(n_pc):\n",
        "                sum_vec = sum_vec + np.dot(cluster_representations[i][index],\n",
        "                          np.transpose(component)[:,j].reshape((768,1))) * component[j]\n",
        "        \n",
        "        post_rep[index]=cluster_representations[i][index] - sum_vec\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  return post_rep\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRaD2KdCNBQ"
      },
      "source": [
        "# @title Getting representations\n",
        "\n",
        "representation_dev = []\n",
        "for i in range(len(df_dev)):\n",
        "    print(i)\n",
        "    #First sentence\n",
        "    inputs = tokenizer.encode(df_dev['sentence1'].iloc[i], add_special_tokens=True)\n",
        "    inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\n",
        "\n",
        "    #getting the representation of the last layer\n",
        "    output = model(inputs)[0]\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    #Removing CLS and SEP tokens\n",
        "    idx = [0, len(output)-1]\n",
        "    output = np.delete(output, idx, axis= 0)\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    representation_dev.append(output)\n",
        "\n",
        "    #Second sentence\n",
        "    inputs = tokenizer.encode(df_dev['sentence2'].iloc[i], add_special_tokens=True)\n",
        "    inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\n",
        "\n",
        "    output = model(inputs)[0]\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    #Removing CLS and SEP tokens\n",
        "    idx = [0, len(output)-1]\n",
        "    output = np.delete(output, idx, axis= 0)\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    representation_dev.append(output)\n",
        "\n",
        "representation_list_dev=[]\n",
        "for i in range(len(representation_dev)):\n",
        "  for j in range(len(representation_dev[i])):\n",
        "      representation_list_dev.append(representation_dev[i][j])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSUmN7jRxhe"
      },
      "source": [
        "# making the representations isotorpic\n",
        "n_cluster = 27\n",
        "n_pc = 12\n",
        "isotropic_representations = cluster_based(np.asarray(representation_list_dev),\n",
        "                                          n_cluster, n_pc)\n",
        "\n",
        "# calculating sentence representations\n",
        "sentence_rep = mean_pooling(isotropic_representations, representation_dev)\n",
        "\n",
        "# predicting similarity scores\n",
        "score = similarity(sentence_rep)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7TsU2AFcFuF",
        "outputId": "724575b6-8ba6-4764-ee19-7d491f0920bc"
      },
      "source": [
        "# performance\n",
        "print(\"Spearman Correlation: \",sper_corrcoef(df_dev['score'], score))\n",
        "\n",
        "# isotropy of space\n",
        "print(\"Isotropy: \", isotropy(isotropic_representations))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman Correlation:  74.8463511184579\n",
            "Isotropy:  0.7506211880816394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_fRT5DcvYq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}