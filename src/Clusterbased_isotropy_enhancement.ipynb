{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clusterbased_isotropy_enhancement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "auwHg2y-_4Hz"
      },
      "source": [
        "# @title Install Transformers\n",
        "from IPython.display import clear_output\n",
        "!pip install transformers==2.11\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVuK2H63AYhp"
      },
      "source": [
        "# @title Import Requirements\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import scipy as sc\n",
        "import math as mt\n",
        "from scipy import cluster as clst\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bxzwn8tAVcL"
      },
      "source": [
        "# @title Loading BERT\n",
        "casing = \"bert-base-uncased\" \n",
        "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\n",
        "\n",
        "config = BertConfig(dropout=0.2, attention_dropout=0.2, )\n",
        "config.output_hidden_states = True\n",
        "\n",
        "model = TFBertModel.from_pretrained(casing, config = config)\n",
        "model.trainable = False\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtU8arrBBMVj"
      },
      "source": [
        "# @title Loading Dataset\n",
        "\n",
        "# STS benchmark\n",
        "#df_train = pd.read_csv('train.tsv', delimiter='\\t' , error_bad_lines=False)\n",
        "df_dev = pd.read_csv('dev.tsv', delimiter='\\t' , error_bad_lines=False)\n",
        "#df_test = pd.read_csv('test.tsv', delimiter='\\t' , error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdRVMSKFCuza"
      },
      "source": [
        "# @title Required functions\n",
        "\n",
        "def sper_corrcoef(targets, predictions):\n",
        "    \"\"\"Spearman correlation coefficient.\"\"\"\n",
        "    return 100 * sc.stats.spearmanr(targets, predictions)[0]\n",
        "\n",
        "\n",
        "def mean_pooling(inp_representations, representation_dev):\n",
        "    \"\"\" calculating sentence representations by averaging over the tokens.\"\"\"\n",
        "\n",
        "    sum_index=0\n",
        "    sent_representations=[]\n",
        "    for i in range(len(representation_dev)):\n",
        "      sent_representations.append(np.mean(inp_representations[sum_index: sum_index + (len(representation_dev[i]))],axis=0))\n",
        "      sum_index = sum_index + len(representation_dev[i])\n",
        "\n",
        "    return sent_representations\n",
        "\n",
        "\n",
        "def similarity(sent_rep):\n",
        "    \"\"\" calculating cosine similarity between two sentences.\"\"\"\n",
        "  \n",
        "    score = []\n",
        "    l = 0\n",
        "    for i in range(int(len(sent_rep)/2)):\n",
        "        score.append(cosine_similarity(np.reshape(sent_rep[l], (1, 768)),\n",
        "                                      np.reshape(sent_rep[l + 1], (1, 768)))[0][0])\n",
        "        l = l + 2\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def isotropy(representations):\n",
        "    \"\"\"Calculating isotropy of embedding space based on Eq.2\n",
        "           arg:\n",
        "              representations (n_samples, n_dimensions)\n",
        "            \"\"\"\n",
        "\n",
        "    eig_values, eig_vectors = np.linalg.eig(np.matmul(np.transpose(representations),\n",
        "                                                      representations))\n",
        "    max_f = -mt.inf\n",
        "    min_f =  mt.inf\n",
        "\n",
        "    for i in range(eig_vectors.shape[1]):\n",
        "        f = np.matmul(representations, np.expand_dims(eig_vectors[:, i], 1))\n",
        "        f = np.sum(np.exp(f))\n",
        "\n",
        "        min_f = min(min_f, f)\n",
        "        max_f = max(max_f, f)\n",
        "\n",
        "    isotropy = min_f / max_f\n",
        "\n",
        "    return isotropy\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONNd5H3bLjhN"
      },
      "source": [
        "# @title Cluster-based Isotropy Enhancement\n",
        "\n",
        "def cluster_based(representations, n_cluster: int, n_pc: int):\n",
        "  \"\"\" Improving Isotropy of input representations using cluster-based method\n",
        "      Args: \n",
        "            inputs:\n",
        "                  representations: \n",
        "                    input representations numpy array(n_samples, n_dimension)\n",
        "                  n_cluster: \n",
        "                    the number of clusters\n",
        "                  n_pc: \n",
        "                    the number of directions to be discarded\n",
        "            output:\n",
        "                  isotropic representations (n_samples, n_dimension)\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "  centroid, label=clst.vq.kmeans2(representations, n_cluster, minit='points',\n",
        "                                  missing='warn', check_finite=True)\n",
        "  cluster_mean=[]\n",
        "  for i in range(max(label)+1):\n",
        "    sum=np.zeros([1,768]);\n",
        "    for j in np.nonzero(label == i)[0]:\n",
        "      sum=np.add(sum, representations[j])\n",
        "    cluster_mean.append(sum/len(label[label == i]))\n",
        "\n",
        "  zero_mean_representation=[]\n",
        "  for i in range(len(representations)):\n",
        "    zero_mean_representation.append((representations[i])-cluster_mean[label[i]])\n",
        "\n",
        "  cluster_representations={}\n",
        "  for i in range(n_cluster):\n",
        "    cluster_representations.update({i:{}})\n",
        "    for j in range(len(representations)):\n",
        "      if (label[j]==i):\n",
        "        cluster_representations[i].update({j:zero_mean_representation[j]})\n",
        "\n",
        "  cluster_representations2=[]\n",
        "  for j in range(n_cluster):\n",
        "    cluster_representations2.append([])\n",
        "    for key, value in cluster_representations[j].items():\n",
        "      cluster_representations2[j].append(value)\n",
        "\n",
        "  cluster_representations2=np.array(cluster_representations2)\n",
        "\n",
        "\n",
        "  model=PCA()\n",
        "  post_rep=np.zeros((representations.shape[0],representations.shape[1]))\n",
        "\n",
        "  for i in range(n_cluster):\n",
        "      model.fit(np.array(cluster_representations2[i]).reshape((-1,768)))\n",
        "      component = np.reshape(model.components_, (-1, 768))\n",
        "\n",
        "      for index in cluster_representations[i]:\n",
        "        sum_vec = np.zeros((1, 768))\n",
        "\n",
        "        for j in range(n_pc):\n",
        "                sum_vec = sum_vec + np.dot(cluster_representations[i][index],\n",
        "                          np.transpose(component)[:,j].reshape((768,1))) * component[j]\n",
        "        \n",
        "        post_rep[index]=cluster_representations[i][index] - sum_vec\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  return post_rep\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRaD2KdCNBQ"
      },
      "source": [
        "# @title Getting representations\n",
        "\n",
        "representation_dev = []\n",
        "for i in range(len(df_dev)):\n",
        "    print(i)\n",
        "    #First sentence\n",
        "    inputs = tokenizer.encode(df_dev['sentence1'].iloc[i], add_special_tokens=True)\n",
        "    inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\n",
        "\n",
        "    #getting the representation of the last layer\n",
        "    output = model(inputs)[0]\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    #Removing CLS and SEP tokens\n",
        "    idx = [0, len(output)-1]\n",
        "    output = np.delete(output, idx, axis= 0)\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    representation_dev.append(output)\n",
        "\n",
        "    #Second sentence\n",
        "    inputs = tokenizer.encode(df_dev['sentence2'].iloc[i], add_special_tokens=True)\n",
        "    inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\n",
        "\n",
        "    output = model(inputs)[0]\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    #Removing CLS and SEP tokens\n",
        "    idx = [0, len(output)-1]\n",
        "    output = np.delete(output, idx, axis= 0)\n",
        "    output = np.asarray(output).reshape((-1,768))\n",
        "\n",
        "    representation_dev.append(output)\n",
        "\n",
        "representation_list_dev=[]\n",
        "for i in range(len(representation_dev)):\n",
        "  for j in range(len(representation_dev[i])):\n",
        "      representation_list_dev.append(representation_dev[i][j])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSUmN7jRxhe"
      },
      "source": [
        "# making the representations isotorpic\n",
        "n_cluster = 27\n",
        "n_pc = 12\n",
        "isotropic_representations = cluster_based(np.asarray(representation_list_dev),\n",
        "                                          n_cluster, n_pc)\n",
        "\n",
        "# calculating sentence representations\n",
        "sentence_rep = mean_pooling(isotropic_representations, representation_dev)\n",
        "\n",
        "# predicting similarity scores\n",
        "score = similarity(sentence_rep)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7TsU2AFcFuF",
        "outputId": "724575b6-8ba6-4764-ee19-7d491f0920bc"
      },
      "source": [
        "# performance\n",
        "print(\"Spearman Correlation: \",sper_corrcoef(df_dev['score'], score))\n",
        "\n",
        "# isotropy of space\n",
        "print(\"Isotropy: \", isotropy(isotropic_representations))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman Correlation:  74.8463511184579\n",
            "Isotropy:  0.7506211880816394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PYAC8qNMag8"
      },
      "source": [
        "#Classification Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_fRT5DcvYq"
      },
      "source": [
        "# @title Loading Data set\n",
        "\n",
        "## BoolQ\n",
        "df_train = pd.read_json('train.jsonl' , lines=True)\n",
        "df_dev = pd.read_json('val.jsonl', lines=True)\n",
        "df_test = pd.read_json('test.jsonl', lines=True)\n",
        "\n",
        "#### training set labels\n",
        "for i in range(len(df_train)):\n",
        "  if df_train['label'].iloc[i] == False:\n",
        "    df_train['label'].iloc[i] = 0\n",
        "  else:\n",
        "    df_train['label'].iloc[i] = 1\n",
        "\n",
        "#### validation set labels\n",
        "for i in range(len(df_dev)):\n",
        "  if df_dev['label'].iloc[i] == False:\n",
        "    df_dev['label'].iloc[i] = 0\n",
        "  else:\n",
        "    df_dev['label'].iloc[i] = 1\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGTwaFYSTnmP"
      },
      "source": [
        "# @title Tokenizer\n",
        "def tokenize(df, tokenizer):\n",
        "    input_ids, input_masks, input_segments, input_offsets = [],[],[],[]\n",
        "    for i in range(len(df)):\n",
        "        inputs = tokenizer.encode_plus(df['question'].iloc[i], df['passage'].iloc[i], add_special_tokens=True, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True, max_length = 64,pad_to_max_length=True )\n",
        "\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])  \n",
        "\n",
        "    return tf.cast(input_ids,tf.int32),tf.cast(input_masks,tf.int32), tf.cast(input_segments,tf.int32) "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb97eLHAk1s3"
      },
      "source": [
        "# @title Making the inputs ready\n",
        "\n",
        "train_ids, train_masks, train_segments = tokenize(df_train, tokenizer)\n",
        "dev_ids, dev_masks, dev_segments = tokenize(df_dev, tokenizer)\n",
        "test_ids, test_masks, test_segments = tokenize(df_test, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgBdep0nVHwb",
        "outputId": "3dca9615-5856-43d6-b959-c86defeaeb99"
      },
      "source": [
        "# @title Getting Representations\n",
        "\n",
        "representation_train = []\n",
        "\n",
        "i = 0\n",
        "while i < len(df_train):\n",
        "    print(i)\n",
        "    if i != 9000:\n",
        "      train_inp = [train_ids[i:i + 1000], train_masks[i : i +1000], train_segments[i : i + 1000]]\n",
        "    else:\n",
        "      train_inp = [train_ids[i:], train_masks[i :], train_segments[i :]]\n",
        "    output = model(train_inp)[0]\n",
        "    output = np.asarray(output).reshape((-1,64,768))\n",
        "    for j in range(output.shape[0]):\n",
        "        representation_train.append(output[j].reshape(64,768))\n",
        "    i = i + 1000\n",
        "\n",
        "clear_output()\n",
        "print(\"done!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ-J3oFWV7YF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1addaca2-efa4-497d-f322-cae98bfe5d75"
      },
      "source": [
        "# @title Getting Representations\n",
        "\n",
        "representation_dev = []\n",
        "i = 0\n",
        "\n",
        "while i < len(df_dev):\n",
        "    print(i)\n",
        "    if i != 3000:\n",
        "      dev_inp = [dev_ids[i:i + 1000], dev_masks[i : i +1000], dev_segments[i : i + 1000]]\n",
        "    else:\n",
        "      dev_inp = [dev_ids[i:], dev_masks[i :], dev_segments[i :]]\n",
        "    output = model(dev_inp)[0]\n",
        "    output = np.asarray(output).reshape((-1,64,768))\n",
        "    for j in range(output.shape[0]):\n",
        "        representation_dev.append(output[j].reshape(64,768))\n",
        "    i = i + 1000\n",
        "\n",
        "clear_output()\n",
        "print(\"done!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPalidIaVmoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e642c0e3-270f-4b76-e239-619b97fb7f9b"
      },
      "source": [
        "# @title Getting Representations\n",
        "\n",
        "representation_test = []\n",
        "i = 0\n",
        "\n",
        "while i < len(df_test):\n",
        "    print(i)\n",
        "    if i != 3000:\n",
        "      test_inp = [test_ids[i:i + 1000], test_masks[i : i +1000], test_segments[i : i + 1000]]\n",
        "    else:\n",
        "      test_inp = [test_ids[i:], test_masks[i :], test_segments[i :]]\n",
        "    output = model(test_inp)[0]\n",
        "    output = np.asarray(output).reshape((-1,64,768))\n",
        "    for j in range(output.shape[0]):\n",
        "        representation_test.append(output[j].reshape(64,768))\n",
        "    i = i + 1000\n",
        "\n",
        "clear_output()\n",
        "print(\"done!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnFpQMSFYcKB"
      },
      "source": [
        "# @title Again Data!\n",
        "train_data = np.reshape(representation_train, (-1, 64, 768))\n",
        "dev_data = np.reshape(representation_dev, (-1, 64, 768))\n",
        "# test_data = np.reshape(representation_test, (-1, 64, 768))\n",
        "\n",
        "train_label = np.asarray(list(df_train['label']))\n",
        "dev_label = np.asarray(list(df_dev['label']))\n",
        "train_label.shape"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4epUi0VWYdp4"
      },
      "source": [
        "# @title Building the MLP\n",
        "\n",
        "inp = tf.keras.layers.Input(shape=(64,768))\n",
        "out = tf.keras.layers.Flatten()(inp)\n",
        "out = tf.keras.layers.Dense(100, activation='relu')(out)\n",
        "out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
        "model = tf.keras.Model(inputs = inp , outputs = out)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer= optimizer,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bu2zmMrYv3h"
      },
      "source": [
        "# @title Checkpoint to save the best model\n",
        "\n",
        "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, monitor, save_path):\n",
        "    super(ModelCheckpoint, self).__init__()\n",
        "    self.monitor = monitor\n",
        "    self.save_path = save_path\n",
        "    self.bestScore = -np.Inf\n",
        "    self.bestLoss = np.Inf\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    score = logs.get(self.monitor)\n",
        "    loss = logs.get(\"val_loss\")\n",
        "    if score > self.bestScore or (score == self.bestScore and loss < self.bestLoss):\n",
        "      path = os.path.join(SAVED_MODELS_DIR, str(epoch+1))\n",
        "      os.makedirs(path)\n",
        "      self.model.save_weights(path+'/best_weights.h5')\n",
        "      self.bestScore = score\n",
        "      self.bestLoss = loss\n",
        "      print(\"\\nModel saved as the best model\")\n",
        "\n",
        "monitor = \"val_acc\"\n",
        "SAVED_MODELS_DIR = '/content/saved_models/'\n",
        "checkpoint = ModelCheckpoint(monitor, SAVED_MODELS_DIR)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0h0aP5HYzah",
        "outputId": "8a0b2a1f-982a-456b-9e7e-a6227da947b2"
      },
      "source": [
        "# @title Training the model\n",
        "history = model.fit(train_data, train_label, epochs = 10, validation_data=(dev_data, dev_label), batch_size = 32, callbacks=[checkpoint] )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.6578 - acc: 0.6249 - val_loss: 0.6360 - val_acc: 0.6419\n",
            "\n",
            "Model saved as the best model\n",
            "Epoch 2/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.5386 - acc: 0.7268 - val_loss: 0.6377 - val_acc: 0.6572\n",
            "\n",
            "Model saved as the best model\n",
            "Epoch 3/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.3995 - acc: 0.8444 - val_loss: 0.6422 - val_acc: 0.6569\n",
            "Epoch 4/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2756 - acc: 0.9281 - val_loss: 0.6632 - val_acc: 0.6560\n",
            "Epoch 5/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.1896 - acc: 0.9674 - val_loss: 0.6948 - val_acc: 0.6391\n",
            "Epoch 6/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.1322 - acc: 0.9809 - val_loss: 0.7299 - val_acc: 0.6517\n",
            "Epoch 7/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0986 - acc: 0.9822 - val_loss: 0.7599 - val_acc: 0.6471\n",
            "Epoch 8/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0783 - acc: 0.9836 - val_loss: 0.7878 - val_acc: 0.6495\n",
            "Epoch 9/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0675 - acc: 0.9842 - val_loss: 0.8261 - val_acc: 0.6456\n",
            "Epoch 10/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0593 - acc: 0.9857 - val_loss: 0.8488 - val_acc: 0.6404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eIoTXv4bswY"
      },
      "source": [
        "# @title Making the representations isotropic\n",
        "n_cluster = 27\n",
        "n_pc = 12\n",
        "representation = list(train_data.reshape(-1, 768)) + list(dev_data.reshape(-1, 768)) \n",
        "isotropic_representations = cluster_based(np.asarray(representation),\n",
        "                                          n_cluster, n_pc)\n",
        "\n",
        "isotropic_train_data = isotropic_representations[: len(list(train_data.reshape(-1, 768)))]\n",
        "isotropic_dev_data = isotropic_representations[len(list(train_data.reshape(-1, 768))): ]\n",
        "\n",
        "isotropic_train_data = np.asarray(isotropic_train_data).reshape(-1, 64, 768)\n",
        "isotropic_dev_data = np.asarray(isotropic_dev_data).reshape(-1, 64, 768)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo_XAHrefSA1",
        "outputId": "aeb8d195-a3e3-4f5a-8ab5-3e7170dd7950"
      },
      "source": [
        "# @title Training the model\n",
        "\n",
        "history = model.fit(isotropic_train_data, train_label, epochs = 10, \n",
        "                    validation_data=(isotropic_dev_data, dev_label), batch_size = 32, callbacks=[checkpoint] )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "295/295 [==============================] - 3s 7ms/step - loss: 0.6447 - acc: 0.6320 - val_loss: 0.6181 - val_acc: 0.6624\n",
            "\n",
            "Model saved as the best model\n",
            "Epoch 2/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.3749 - acc: 0.8897 - val_loss: 0.6213 - val_acc: 0.6602\n",
            "Epoch 3/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2365 - acc: 0.9681 - val_loss: 0.6365 - val_acc: 0.6639\n",
            "\n",
            "Model saved as the best model\n",
            "Epoch 4/10\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.1550 - acc: 0.9835 - val_loss: 0.6593 - val_acc: 0.6648\n",
            "\n",
            "Model saved as the best model\n",
            "Epoch 5/10\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.1082 - acc: 0.9843 - val_loss: 0.6902 - val_acc: 0.6584\n",
            "Epoch 6/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0819 - acc: 0.9849 - val_loss: 0.7231 - val_acc: 0.6575\n",
            "Epoch 7/10\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.0643 - acc: 0.9871 - val_loss: 0.7512 - val_acc: 0.6645\n",
            "Epoch 8/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0547 - acc: 0.9865 - val_loss: 0.7817 - val_acc: 0.6618\n",
            "Epoch 9/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0469 - acc: 0.9876 - val_loss: 0.8095 - val_acc: 0.6578\n",
            "Epoch 10/10\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.0408 - acc: 0.9896 - val_loss: 0.8405 - val_acc: 0.6554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3sME8CMZyYo"
      },
      "source": [
        "# @title Retrieving the best model\n",
        "\n",
        "list_of_dirs = os.listdir('/content/saved_models/')\n",
        "\n",
        "final_list = list(map(int, list_of_dirs))\n",
        "best_model = max(final_list)\n",
        "\n",
        "model_path =\"/content/saved_models/\"+ str(best_model) + '/best_weights.h5'\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}